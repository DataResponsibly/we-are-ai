---
layout: page
---

## Glossary of Terms

**Algorithm**  is a procedure—a sequence of steps—that allows us to solve a problem without having to reinvent a solution each time. Getting ready for work or school in the morning, cooking great chicken soup, getting the fastest directions to a destination, or troubleshooting the check-engine light in your car—these are all algorithms. 

**Algorithmic morality** is the act of attributing moral reasoning to algorithms.

**Artificial Intelligence (AI):** We get to Artificial Intelligence when steps and goals (an algorithm) are used by a computer (artificial) to make decisions (intelligence) using data. This is how AI powers much of the technology we have come to rely upon in modern life. The key thing to remember is that Artificial Intelligence is always derived from the “Everyday Intelligence” that all human beings possess. Which AI is “best” depends on the goals we have for it and on its context of use. 

**Classifier** is an algorithm that makes predictions about the world by assigning labels to observations. Very often, predictions made by classifiers are then turned into actions, and so they affect the world.  For example, a smart light classifier decides whether to turn the light on or off.  This AI will automate a task that is usually done by a human, and we will say that this AI works well if it is able to guess whether a human would have turned the light on or off.  

**Data** is commonly used as another word for information. Oftentimes data is gathered in a specific format suitable for use on a computer (e.g., a spreadsheet). Data is a crucial ingredient in many artificial intelligence systems.

**Rule-based algorithms** are fully prescribed algorithms that follow pre-defined rules. For example, a rule-based algorithm for making sourdough bread might look like a recipe from a cookbook, listing which ingredients are needed, how much of each ingredient to mix in what order, and how long to bake.

**Learning algorithms**, also known as **data-driven algorithms**, are algorithms that determine their own procedure for a given task using data that we provide on previous inputs and desired outputs. Learning algorithms take the data we provide as experience and learn a mapping from input to output, often under constraints that we define. They can then produce new outputs given future inputs. For example, a learning algorithm for making sourdough bread might take several recipes as input, along with information on whether or not each recipe is tasty, and learn which combinations of ingredients make good sourdough.

**Machine learning** is a specific set of algorithms and approaches, all of which could be considered AI. AI is a broad term that includes lots of algorithms that make machines seem smart. Machine learning is a narrower term that includes algorithms that rely on data and statistics to do pattern matching and recognition. We won’t be too concerned about the difference in this course, but it's helpful to know both terms, and which one is “bigger!”

**Stakeholder** is an individual, a group, or an organization that may be affected by a decision, directly or indirectly.  These effects can be positive or negative, depending on stakeholders’ values.

**The trolley problem** is an ethical dilemma that illustrates the principles of utilitarianism. It involves making decisions about whether to sacrifice the life of one person to save another person, or a group of people.

**Utilitarianism** is a moral principle that holds that the right course of action in any situation is the one that produces the greatest balance of benefits over harms for everyone affected. According to this principle, an action is right if it promotes happiness or pleasure, and it is wrong if it produces unhappiness or pain—not just for the performer of the action but also for everyone else affected by it. Utilitarianism stems from the late 18th- and 19th-century English philosophers and economists Jeremy Bentham and John Stuart Mill.

## Additional Terms

**Computer vision** includes any algorithm (usually an AI algorithm) that provides some meaningful interpretation of image based data - that could be a photo, video or even an MRI scan!

**Deep learning** is an even more specific set of algorithms than machine learning, and encompasses any approach to learning or problem solving that relies on really, really big neural networks. We define those below!

**Natural Language Processing** (NLP) includes any algorithm that uses text as its input or output, and seeks to replicate (in some way) the human ability to understand language.

**Neural networks** allow us to detect complex patterns. Unlike simpler algorithms, if we feed data through a neural network, we may be able to more accurately differentiate certain examples (like cat photos) from other examples (like rocks). Why? Neural networks are often collections of many, many different settings, all of which are connected. Using really clever update algorithms, we can adjust the settings so that when we show the network a photo of a cat, it produces a slightly different output than it would for a photo of a lion, and a really different output than for a photo of a big rock. Because there are so many settings, the network is super flexible, and can “learn” how to detect many different things!

**Supervised learning** is a type of AI, or machine learning, that operates on data. With supervised learning, we have a specific question in mind, like “Does this photo have a cat in it?” To answer this question with a computer, we collect lots of photos, many with cats and many without. This is our data, which we can use to train our algorithm. The most important part of supervised learning is that we understand the task fully, and can thus label each image ourselves, “Has cat” or “No cat.” We can then feed this labeled training data through a supervised learning algorithm, which learns to apply this same label to new images.

**Unsupervised learning** is, like supervised learning, a type of AI or machine learning that operates on data. However, with unsupervised learning, we don’t need to have a specific question in mind. We don’t need to label any data. Consider our example with photos before. Instead of labeling them “Has cat” or “No cat,” we may want to cluster this same collection of photos. Here, by cluster, we mean we want to give these photos to an algorithm that will determine which photos are alike and which are dissimilar. We expect that the similar photos will “cluster” together. Importantly, we do not have to explicitly define a label for these clusters! However, we may learn new things: for example, cat photos may be in a cluster together that also includes lions and tigers, but doesn’t include dogs or people.
