---
title: Exploring Bias 
---

## Exploring Bias (30 mins)

<br>

#### Group Discussion (5 min)

In Module 2, we discussed how machines learn from data to make decisions on our behalf. This module explores how those decisions (which can have great impact) can be influenced by human bias.

With the full group or in breakout sessions, spent 5 minutes discussing the following: 
* When you think about or hear the term bias, what comes to mind?
* Considering your own experience or understanding of bias, how can you imagine bias positively or negatively impacting the data and technologies we've been exploring? 

<br>

#### Watch "Bias in the Data” (12 min)

<div class="embed-responsive embed-responsive-16by9">
  <iframe class="embed-responsive-item" src="https://drive.google.com/file/d/18Ioh_7HhBnS6y_no8CD0x5yYbP4pZwXu/preview">
  </iframe></div>
 
**Key terms:** pre-existing bias, technical bias, emergent bias

**References:**
* MIT Technology Review, “Racism is poisoning online ad delivery, says Harvard professor”, February 4, 2013 ([link](https://www.technologyreview.com/2013/02/04/253879/racism-is-poisoning-online-ad-delivery-says-harvard-professor/))
* Wall Street Journal, “Are workplace personality tests fair?”, September 29, 2014 ([link](http://www.wsj.com/articles/are-workplace-personality-tests-fair-1412044257))
* The Guardian, “Women less likely to be shown ads for high-paid jobs on Google, study shows”, July 8, 2015 ([link](https://www.theguardian.com/technology/2015/jul/08/women-less-likely-ads-high-paid-jobs-google-study))
* Reuters, “Amazon scraps secret AI recruiting tool that showed bias against women”, October 10, 2018 ([link](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G))

<br>

#### Group Discussion (10 min)
Once you’ve completed the video, gather the group or split into breakout sessions to discuss some of the real-world scenarios shared in the video.

**Gender Disparity in Amazon Recruiting**

In late 2018, it was discovered that Amazon’s AI recruiting tool, originally designed to boost workforce diversity, had taught itself to prioritize male candidates. The system was trained on historical data, using performance indicators from candidates who were hired through traditional means.

* Where do you see potential for bias in the data used by Amazon’s resume screening AI? Consider how overrepresentation, underrepresentation, or other kinds of distortion might play a role in how this machine learned.
* If you were hired to build a system to diversify a workplace, what approach would you take? What data would you use to define and highlight highly-qualified candidates?

**Discussion: Racism in Online Ads**

In 2013, Latanya Sweeney, a computer science professor at Harvard, noticed that searching Google for African-American-sounding names was more likely to display ads suggesting criminal records than searching for white-sounding names, regardless of if the individual had a record or not.

* How can these types of ads cause harm? Who is likely to be affected?
* Who benefits when these ads are shown?
* Who should be responsible for mitigating the harms?

The (financial) benefit is to Google’s ad serving platform and to the advertiser, Instant Checkmate, because they get higher click-through rates when these ads are shown in response to African American sounding names. Part of the reason for this is that employers are more likely to believe the ads in the case of an African American applicant, and are therefore more likely to click. Google is in the best position to mitigate - they benefit the most, and they are holding all the cards in terms of access to information about how their ad targeting platform works (we can only guess).
